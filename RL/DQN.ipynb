{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d0a7153",
   "metadata": {},
   "source": [
    "DQN主要包含：\n",
    "\n",
    "1. **两个网络（策略网络和目标网络）**：这两个网络架构一模一样，初始化时参数也相同。但在训练过程中，策略网络每步都更新；目标网络在一定步数后才更新一次（直接将策略网络的参数复制过去）。这样做是为了稳定训练，防止Q值估计的发散。\n",
    "\n",
    "**目标函数**：DQN使用均方误差（Mean Squared Error, MSE）作为目标函数，计算当前Q值与目标Q值之间的差异。目标Q值由奖励和下一个状态的最大Q值计算得出，具体公式为：\n",
    "\n",
    "$$\n",
    "y = r + \\gamma \\max_{a'} Q_{\\text{target}}(s', a')\n",
    "$$\n",
    "\n",
    "其中，$y$ 是目标Q值，$r$ 是即时奖励，$\\gamma$ 是折扣因子，$s'$ 是下一个状态，$Q_{\\text{target}}$ 是目标网络的Q值。\n",
    "\n",
    "**损失函数**：DQN的损失函数定义为当前Q值与目标Q值之间的均方误差：\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\mathbb{E}_{(s, a, r, s') \\sim D} \\left[ \\left( y - Q_{\\text{policy}}(s, a; \\theta) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "其中，$\\theta$ 是策略网络的参数，$D$ 是经验回放缓冲区中的数据分布。\n",
    "\n",
    "1. **经验回放（Experience Replay）**：DQN使用一个经验回放缓冲区来存储智能体与环境交互的历史数据（状态、动作、奖励、下一个状态）。在训练时，从这个缓冲区随机采样小批量数据进行训练，打破数据之间的相关性，提高训练效率和稳定性。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3684de36",
   "metadata": {},
   "source": [
    "## 网络\n",
    "网络的输入是**状态**，大小是**状态的数量**。\n",
    "\n",
    "输出的是在采取该动作后的Q值，输出大小跟**动作数**一样\n",
    "\n",
    "要考虑**终止状态**和**非终止状态**的区别\n",
    "\n",
    "$$\n",
    "y_i = r + γ max_a' Q_target(s', a')  (非终止状态)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_i = r  (终止状态)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea40835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "用简单的线性网络来实现DQN的网络结构\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_states,n_actions,hidden_dim=128):\n",
    "        \"\"\" 初始化q网络，为全连接网络\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_states, hidden_dim) # 输入层\n",
    "        self.fc2 = nn.Linear(hidden_dim,hidden_dim) # 隐藏层\n",
    "        self.fc3 = nn.Linear(hidden_dim, n_actions) # 输出层\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 各层对应的激活函数\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317f39ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定义 经验回放池\n",
    "\"\"\"\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self,capacity):\n",
    "        self.capacity = capacity  # 最大容量\n",
    "        self.buffer = deque(maxlen=self.capacity)\n",
    "\n",
    "    def push(self,transition):\n",
    "        \"\"\"\n",
    "        添加经验到经验回放池中\n",
    "        \"\"\"\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self,batch_size:int, sequential:bool=False):\n",
    "        \"\"\"\n",
    "        从经验回放池中采样一批经验\n",
    "        \"\"\"\n",
    "        if batch_size > len(self.buffer):\n",
    "            # 如果请求的批量大小大于当前缓冲区大小，则调整为当前回放池大小\n",
    "            batch_size = len(self.buffer)\n",
    "        if sequential:\n",
    "            rand = random.randint(0, len(self.buffer) - batch_size) # 随机起始位置\n",
    "            batch = [self.buffer[i] for i in range(rand, rand + batch_size)] # 从起始位置开始连续采样\n",
    "            return zip(*batch)\n",
    "        else:\n",
    "            # 随机采样\n",
    "            indices = random.sample(range(len(self.buffer)), batch_size)\n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "        return map(list, zip(*batch))  # 转置操作，将每个元素分开返回\n",
    "\n",
    "    def clear(self):\n",
    "        ''' 清空经验回放\n",
    "        '''\n",
    "        self.buffer.clear()\n",
    "    def __len__(self):\n",
    "        ''' 返回当前存储的量\n",
    "        '''\n",
    "        return len(self.buffer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DQN算法实现\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, model,memory,cfg):\n",
    "        self.n_actions = cfg.n_actions  # 动作数\n",
    "        self.device = torch.device(cfg.device)\n",
    "        self.gamma = cfg.gamma  # 折扣因子\n",
    "        # epsilon-greedy参数\n",
    "        self.sample_count = 0  # 用于epsilon的衰减计数\n",
    "        self.epsilon = cfg.epsilon_start  # 初始epsilon值\n",
    "        self.sample_count = 0  \n",
    "        self.epsilon_start = cfg.epsilon_start\n",
    "        self.epsilon_end = cfg.epsilon_end\n",
    "        self.epsilon_decay = cfg.epsilon_decay   # 衰减速率\n",
    "        # 训练参数\n",
    "        self.batch_size = cfg.batch_size\n",
    "        self.policy_net = model.to(self.device)  # 策略网络\n",
    "        self.target_net = model.to(self.device)  # 目标网络\n",
    "\n",
    "        # 复制策略网络的参数到目标网络\n",
    "        for target_param, param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=cfg.lr)\n",
    "        self.memory = memory  # 经验回放池\n",
    "    \n",
    "    def sample_action(self,state):\n",
    "        \"\"\"epsilon-greedy策略选择动作\"\"\"\n",
    "        self.sample_count += 1    # 采样次数加1\n",
    "        # 计算当前epsilon值，随着采样次数增加逐渐减小\n",
    "        self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "            math.exp(-1. * self.sample_count / self.epsilon_decay)\n",
    "        \n",
    "        if random.random() > self.epsilon:\n",
    "            # 选择Q值最大的动作\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)  \n",
    "                q_values = self.policy_net(state)  # 策略网络的输出（Q值）\n",
    "                action = q_values.max(1)[1].item() # 选择最大Q值对应的动作\n",
    "        else:\n",
    "            # 随机选择动作\n",
    "            action = random.randrange(self.n_actions)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    @ torch.no_grad() # 在Evaluation时不计算梯度\n",
    "    def predict_action(self,state):\n",
    "        \"\"\"\n",
    "        eval时选择动作用，不使用epsilon-greedy，\n",
    "        直接选择Q值最大的动作\n",
    "        \"\"\"\n",
    "\n",
    "        state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "        q_values = self.policy_net(state)  # 策略网络的输出（Q值）\n",
    "        action = q_values.max(1)[1].item() # 选择最大Q值对应的动作\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        更新策略网络参数\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return  # 经验回放池中样本不足一个batch时，就不更新\n",
    "        \n",
    "        # 采样一个batch的状态，动作，奖励，下一个状态，done标志\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "        # 转换为torch张量\n",
    "        state_batch = torch.tensor(np.array(state_batch), device=self.device, dtype=torch.float)\n",
    "        action_batch = torch.tensor(action_batch, device=self.device).unsqueeze(1)  \n",
    "        reward_batch = torch.tensor(reward_batch, device=self.device, dtype=torch.float)  \n",
    "        next_state_batch = torch.tensor(np.array(next_state_batch), device=self.device, dtype=torch.float)\n",
    "        done_batch = torch.tensor(np.float32(done_batch), device=self.device)\n",
    "\n",
    "        # 计算当前状态（state_batch）下，采取action_batch动作的Q值\n",
    "        q_values = self.policy_net(state_batch).gather(dim=1, index=action_batch)\n",
    "\n",
    "        # 计算下一个状态（next_state_batch）下的最大Q值\n",
    "        next_q_values = self.target_net(next_state_batch).max(1)[0].detach\n",
    "\n",
    "        # 计算目标Q值，对于终止状态，此时done_batch[0]=1, 对应的expected_q_value等于reward\n",
    "        target_q_values = reward_batch + self.gamma * next_q_values * (1 - done_batch)\n",
    "\n",
    "        # 计算损失函数（均方误差）\n",
    "        loss = torch.nn.functional.mse_loss(q_values, target_q_values.unsqueeze(1))\n",
    "\n",
    "        # 优化策略网络\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # clip,防止梯度爆炸\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a69538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "训练代码\n",
    "\"\"\"\n",
    "\n",
    "def train(cfg, env, agent):\n",
    "    print(\"开始训练...\")\n",
    "    rewards = []  # 用于记录每个episode的累计奖励\n",
    "    steps = []    # 用于记录每个episode的步数\n",
    "\n",
    "    for episode in range(cfg.num_episodes):\n",
    "        ep_reward = 0  # 记录当前episode的累计奖励\n",
    "        ep_steps = 0  # 记录当前episode的步数\n",
    "        state = env.reset()  # 重置环境，获取初始状态\n",
    "\n",
    "        for _ in range(cfg.max_steps_per_episode):\n",
    "            action = agent.sample_action(state)  # 选择动作\n",
    "            next_state, reward, done, _ = env.step(action)  # 执行动作，获取即时奖励和下一个状态\n",
    "            agent.memory.push(state, action, reward, next_state, done)  # 存储经验到回放池\n",
    "            agent.update()  # 更新策略网络\n",
    "\n",
    "            state = next_state  # 转移到下一个状态\n",
    "            ep_reward += reward  # 累积奖励\n",
    "            ep_steps += 1  # 步数加1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if (episode + 1) % cfg.target_update_freq == 0:\n",
    "            # 每隔一定episode更新目标网络\n",
    "            for target_param, param in zip(agent.target_net.parameters(), agent.policy_net.parameters()):\n",
    "                target_param.data.copy_(param.data)\n",
    "\n",
    "        rewards.append(ep_reward)\n",
    "        steps.append(ep_steps)\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            # 每10个episode打印一次信息\n",
    "            print(f\"回合：{i_ep+1}/{cfg['train_eps']}，奖励：{ep_reward:.2f}，Epislon：{agent.epsilon:.3f}\")\n",
    "\n",
    "    print(\"训练结束！\")\n",
    "    env.close()\n",
    "    return {\"rewards\": rewards, \"steps\": steps}  # 返回训练结果\n",
    "\n",
    "def test(cfg,env,agent):\n",
    "    print(\"开始测试...\")\n",
    "    rewards = []  # 用于记录每个episode的累计奖励\n",
    "    steps = []    # 用于记录每个episode的步数\n",
    "\n",
    "    for episode in range(cfg.test_episodes):\n",
    "        ep_reward = 0  # 记录当前episode的累计奖励\n",
    "        ep_steps = 0  # 记录当前episode的步数\n",
    "        state = env.reset()  # 重置环境，获取初始状态\n",
    "\n",
    "        for _ in range(cfg.max_steps_per_episode):\n",
    "            action = agent.predict_action(state)  # 这里动作用predict直接选Q值最大而不是sample\n",
    "            next_state, reward, done, _ = env.step(action)  # 执行动作，获取即时奖励和下一个状态\n",
    "\n",
    "            state = next_state  # 转移到下一个状态\n",
    "            ep_reward += reward  # 累积奖励\n",
    "            ep_steps += 1  # 步数加1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(ep_reward)\n",
    "        steps.append(ep_steps)\n",
    "\n",
    "        print(f\"回合：{episode+1}/{cfg.test_episodes}，奖励：{ep_reward:.2f}\")\n",
    "\n",
    "    print(\"测试结束！\")\n",
    "    env.close()\n",
    "    return {\"rewards\": rewards, \"steps\": steps}  # 返回测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb06b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定义环境\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import os\n",
    "def all_seed(env,seed = 1):\n",
    "    ''' 万能的seed函数\n",
    "    '''\n",
    "    env.seed(seed) # env config\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed) # config for CPU\n",
    "    torch.cuda.manual_seed(seed) # config for GPU\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # config for python scripts\n",
    "    # config for cudnn\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "def env_agent_config(cfg):\n",
    "    env = gym.make(cfg['env_name']) # 创建环境\n",
    "    if cfg['seed'] !=0:\n",
    "        all_seed(env,seed=cfg['seed'])\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}\")\n",
    "    cfg.update({\"n_states\":n_states,\"n_actions\":n_actions}) # 更新n_states和n_actions到cfg参数中\n",
    "    model = MLP(n_states, n_actions, hidden_dim = cfg['hidden_dim']) # 创建模型\n",
    "    memory = ReplayBuffer(cfg['memory_capacity'])\n",
    "    agent = DQN(model,memory,cfg)\n",
    "    return env,agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa02a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "训练参数\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def get_args():\n",
    "    \"\"\" 超参数\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"hyperparameters\")      \n",
    "    parser.add_argument('--algo_name',default='DQN',type=str,help=\"name of algorithm\")\n",
    "    parser.add_argument('--env_name',default='CartPole-v0',type=str,help=\"name of environment\")\n",
    "    parser.add_argument('--train_eps',default=200,type=int,help=\"episodes of training\")\n",
    "    parser.add_argument('--test_eps',default=20,type=int,help=\"episodes of testing\")\n",
    "    parser.add_argument('--ep_max_steps',default = 100000,type=int,help=\"steps per episode, much larger value can simulate infinite steps\")\n",
    "    parser.add_argument('--gamma',default=0.95,type=float,help=\"discounted factor\")\n",
    "    parser.add_argument('--epsilon_start',default=0.95,type=float,help=\"initial value of epsilon\")\n",
    "    parser.add_argument('--epsilon_end',default=0.01,type=float,help=\"final value of epsilon\")\n",
    "    parser.add_argument('--epsilon_decay',default=500,type=int,help=\"decay rate of epsilon, the higher value, the slower decay\")\n",
    "    parser.add_argument('--lr',default=0.0001,type=float,help=\"learning rate\")\n",
    "    parser.add_argument('--memory_capacity',default=100000,type=int,help=\"memory capacity\")\n",
    "    parser.add_argument('--batch_size',default=64,type=int)\n",
    "    parser.add_argument('--target_update',default=4,type=int)\n",
    "    parser.add_argument('--hidden_dim',default=256,type=int)\n",
    "    parser.add_argument('--device',default='cpu',type=str,help=\"cpu or cuda\") \n",
    "    parser.add_argument('--seed',default=10,type=int,help=\"seed\")   \n",
    "    args = parser.parse_args([])\n",
    "    args = {**vars(args)}  # 转换成字典类型    \n",
    "    ## 打印超参数\n",
    "    print(\"超参数\")\n",
    "    print(''.join(['=']*80))\n",
    "    tplt = \"{:^20}\\t{:^20}\\t{:^20}\"\n",
    "    print(tplt.format(\"Name\", \"Value\", \"Type\"))\n",
    "    for k,v in args.items():\n",
    "        print(tplt.format(k,v,str(type(v))))   \n",
    "    print(''.join(['=']*80))      \n",
    "    return args\n",
    "def smooth(data, weight=0.9):  \n",
    "    '''用于平滑曲线，类似于Tensorboard中的smooth曲线\n",
    "    '''\n",
    "    last = data[0] \n",
    "    smoothed = []\n",
    "    for point in data:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # 计算平滑值\n",
    "        smoothed.append(smoothed_val)                    \n",
    "        last = smoothed_val                                \n",
    "    return smoothed\n",
    "\n",
    "def plot_rewards(rewards,cfg, tag='train'):\n",
    "    ''' 画图\n",
    "    '''\n",
    "    sns.set()\n",
    "    plt.figure()  # 创建一个图形实例，方便同时多画几个图\n",
    "    plt.title(f\"{tag}ing curve on {cfg['device']} of {cfg['algo_name']} for {cfg['env_name']}\")\n",
    "    plt.xlabel('epsiodes')\n",
    "    plt.plot(rewards, label='rewards')\n",
    "    plt.plot(smooth(rewards), label='smoothed')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af3829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "训练\n",
    "\"\"\"\n",
    "\n",
    "# 获取参数\n",
    "cfg = get_args() \n",
    "# 训练\n",
    "env, agent = env_agent_config(cfg)\n",
    "res_dic = train(cfg, env, agent)\n",
    " \n",
    "plot_rewards(res_dic['rewards'], cfg, tag=\"train\")  \n",
    "# 测试\n",
    "res_dic = test(cfg, env, agent)\n",
    "plot_rewards(res_dic['rewards'], cfg, tag=\"test\")  # 画出结果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
