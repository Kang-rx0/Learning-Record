{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4dd955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\clip\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\ProgramData\\miniconda3\\envs\\clip\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\miniconda3\\envs\\clip\\lib\\site-packages\\huggingface_hub\\file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Fetching 10 files:  60%|██████    | 6/10 [00:02<00:01,  2.75it/s]Error while downloading from https://hf-mirror.com/Qwen/Qwen3-0.6B/resolve/c1899de289a04d12100db370d81485cdf75e47ca/tokenizer.json: HTTPSConnectionPool(host='cas-bridge.xethub.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://hf-mirror.com/Qwen/Qwen3-0.6B/resolve/c1899de289a04d12100db370d81485cdf75e47ca/tokenizer.json: HTTPSConnectionPool(host='cas-bridge.xethub.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://hf-mirror.com/Qwen/Qwen3-0.6B/resolve/c1899de289a04d12100db370d81485cdf75e47ca/tokenizer.json: HTTPSConnectionPool(host='cas-bridge.xethub.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://hf-mirror.com/Qwen/Qwen3-0.6B/resolve/c1899de289a04d12100db370d81485cdf75e47ca/tokenizer.json: HTTPSConnectionPool(host='cas-bridge.xethub.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://hf-mirror.com/Qwen/Qwen3-0.6B/resolve/c1899de289a04d12100db370d81485cdf75e47ca/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://hf-mirror.com/Qwen/Qwen3-0.6B/resolve/c1899de289a04d12100db370d81485cdf75e47ca/tokenizer.json: HTTPSConnectionPool(host='cas-bridge.xethub.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://hf-mirror.com/Qwen/Qwen3-0.6B/resolve/c1899de289a04d12100db370d81485cdf75e47ca/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://hf-mirror.com/Qwen/Qwen3-0.6B/resolve/c1899de289a04d12100db370d81485cdf75e47ca/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://hf-mirror.com/Qwen/Qwen3-0.6B/resolve/c1899de289a04d12100db370d81485cdf75e47ca/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://hf-mirror.com/Qwen/Qwen3-0.6B/resolve/c1899de289a04d12100db370d81485cdf75e47ca/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# model_id=\"Qwen/Qwen3-0.6B\"\n",
    "# # repo_id 模型id\n",
    "# # local_dir 下载地址\n",
    "# # endpoint 镜像地址\n",
    "# # resume_download (中断后)继续下载\n",
    "# snapshot_download(repo_id=model_id, local_dir=\"E:\\pythonCode\\Qwen\\model\",\n",
    "#                   local_dir_use_symlinks=False, revision=\"main\",\n",
    "#                   endpoint='https://hf-mirror.com',\n",
    "#                   resume_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60aac942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\clip\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer and the model\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"E:\\pythonCode\\Qwen\\model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12566ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Give me a short introduction to large language model.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "messages[0][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d36d375e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nGive me a short introduction to large language model.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # 切换是否为思考模式\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)  # 放到device里，在上一栏的model里定义 （auto）\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14cd4574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644,    872,    198,  35127,    752,    264,   2805,  16800,    311,\n",
       "           3460,   4128,   1614,     13, 151645,    198, 151644,  77091,    198,\n",
       "         151667,    198,  32313,     11,    279,   1196,   6801,    264,   2805,\n",
       "          16800,    311,    264,   3460,   4128,   1614,     13,   6771,    752,\n",
       "           1191,    553,  88646,   1128,    358,   1414,    911,    444,  10994,\n",
       "             82,     13,   2379,   2299,   2409,   4128,   4119,     11,   1290,\n",
       "             30,   2055,    358,   1265,   6286,    862,   6200,   4419,   1156,\n",
       "             13,  10696,   1191,    448,    330,  34253,   4128,   4119,    320,\n",
       "           4086,  21634,   9940,    323,  10339,    862,  16928,    382,     40,\n",
       "           1184,    311,  11167,   1376,  13566,   1075,  23163,   1467,     11,\n",
       "           8660,   2266,     11,    323,  11589,   6351,   9079,     13,   7281,\n",
       "             11,   6286,    429,    807,   2299,  16176,    389,  12767,  29425,\n",
       "             13,  12260,    358,   2924,   2494,    911,    862,   8357,     30,\n",
       "           8909,   4378,     11,   3412,     11,    476,  11521,   9079,     30,\n",
       "           2938,   3643,    279,  16800,    803,  15817,    382,  14190,     11,\n",
       "            279,   1196,   3207,    944,  13837,    421,    807,   1184,  10916,\n",
       "           3793,    476,    264,    803,   4586,  16148,     13,   8704,    432,\n",
       "            594,    264,   2805,  19706,     11,  10282,    432,  63594,    374,\n",
       "           2989,     13,  34006,    502,  70821,    421,   3204,     13,  10696,\n",
       "            912,    264,  11652,    911,    862,  10515,   2897,    323,  59012,\n",
       "             13,   6771,    752,   1779,    421,    358,   2776,   7402,    894,\n",
       "           2989,   3501,     13,   8670,     11,   1083,     11,  10339,    429,\n",
       "            807,    646,    387,   1483,    369,   5257,   9079,     13,  97593,\n",
       "             11,    429,   1265,   3421,    432,    624, 151668,    271,     32,\n",
       "           3460,   4128,   1614,    320,   4086,     44,      8,    374,    264,\n",
       "            943,    315,  20443,  11229,   1849,   6188,    311,   3535,    323,\n",
       "           6923,   3738,  12681,   1467,     13,   4220,   4119,    525,  16176,\n",
       "            389,  10951,  29425,    311,  57289,   6351,   4128,     11,   3535,\n",
       "           2266,     11,    323,   2736,   9079,   1741,    438,   4378,     11,\n",
       "           3412,     11,    323,  11521,   7274,     13,   2379,   7283,   3847,\n",
       "            311,  16282,    448,   4128,    304,    264,   5810,    323,  11050,\n",
       "           1616,     11,   3259,   1105,  55093,    369,   9079,  23994,    504,\n",
       "           2213,   9688,    311,   3491,  98146,     13, 151645]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae4fb55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151644,\n",
       " 872,\n",
       " 198,\n",
       " 35127,\n",
       " 752,\n",
       " 264,\n",
       " 2805,\n",
       " 16800,\n",
       " 311,\n",
       " 3460,\n",
       " 4128,\n",
       " 1614,\n",
       " 13,\n",
       " 151645,\n",
       " 198,\n",
       " 151644,\n",
       " 77091,\n",
       " 198,\n",
       " 151667,\n",
       " 198,\n",
       " 32313,\n",
       " 11,\n",
       " 279,\n",
       " 1196,\n",
       " 6801,\n",
       " 264,\n",
       " 2805,\n",
       " 16800,\n",
       " 311,\n",
       " 264,\n",
       " 3460,\n",
       " 4128,\n",
       " 1614,\n",
       " 13,\n",
       " 6771,\n",
       " 752,\n",
       " 1191,\n",
       " 553,\n",
       " 88646,\n",
       " 1128,\n",
       " 358,\n",
       " 1414,\n",
       " 911,\n",
       " 444,\n",
       " 10994,\n",
       " 82,\n",
       " 13,\n",
       " 2379,\n",
       " 2299,\n",
       " 2409,\n",
       " 4128,\n",
       " 4119,\n",
       " 11,\n",
       " 1290,\n",
       " 30,\n",
       " 2055,\n",
       " 358,\n",
       " 1265,\n",
       " 6286,\n",
       " 862,\n",
       " 6200,\n",
       " 4419,\n",
       " 1156,\n",
       " 13,\n",
       " 10696,\n",
       " 1191,\n",
       " 448,\n",
       " 330,\n",
       " 34253,\n",
       " 4128,\n",
       " 4119,\n",
       " 320,\n",
       " 4086,\n",
       " 21634,\n",
       " 9940,\n",
       " 323,\n",
       " 10339,\n",
       " 862,\n",
       " 16928,\n",
       " 382,\n",
       " 40,\n",
       " 1184,\n",
       " 311,\n",
       " 11167,\n",
       " 1376,\n",
       " 13566,\n",
       " 1075,\n",
       " 23163,\n",
       " 1467,\n",
       " 11,\n",
       " 8660,\n",
       " 2266,\n",
       " 11,\n",
       " 323,\n",
       " 11589,\n",
       " 6351,\n",
       " 9079,\n",
       " 13,\n",
       " 7281,\n",
       " 11,\n",
       " 6286,\n",
       " 429,\n",
       " 807,\n",
       " 2299,\n",
       " 16176,\n",
       " 389,\n",
       " 12767,\n",
       " 29425,\n",
       " 13,\n",
       " 12260,\n",
       " 358,\n",
       " 2924,\n",
       " 2494,\n",
       " 911,\n",
       " 862,\n",
       " 8357,\n",
       " 30,\n",
       " 8909,\n",
       " 4378,\n",
       " 11,\n",
       " 3412,\n",
       " 11,\n",
       " 476,\n",
       " 11521,\n",
       " 9079,\n",
       " 30,\n",
       " 2938,\n",
       " 3643,\n",
       " 279,\n",
       " 16800,\n",
       " 803,\n",
       " 15817,\n",
       " 382,\n",
       " 14190,\n",
       " 11,\n",
       " 279,\n",
       " 1196,\n",
       " 3207,\n",
       " 944,\n",
       " 13837,\n",
       " 421,\n",
       " 807,\n",
       " 1184,\n",
       " 10916,\n",
       " 3793,\n",
       " 476,\n",
       " 264,\n",
       " 803,\n",
       " 4586,\n",
       " 16148,\n",
       " 13,\n",
       " 8704,\n",
       " 432,\n",
       " 594,\n",
       " 264,\n",
       " 2805,\n",
       " 19706,\n",
       " 11,\n",
       " 10282,\n",
       " 432,\n",
       " 63594,\n",
       " 374,\n",
       " 2989,\n",
       " 13,\n",
       " 34006,\n",
       " 502,\n",
       " 70821,\n",
       " 421,\n",
       " 3204,\n",
       " 13,\n",
       " 10696,\n",
       " 912,\n",
       " 264,\n",
       " 11652,\n",
       " 911,\n",
       " 862,\n",
       " 10515,\n",
       " 2897,\n",
       " 323,\n",
       " 59012,\n",
       " 13,\n",
       " 6771,\n",
       " 752,\n",
       " 1779,\n",
       " 421,\n",
       " 358,\n",
       " 2776,\n",
       " 7402,\n",
       " 894,\n",
       " 2989,\n",
       " 3501,\n",
       " 13,\n",
       " 8670,\n",
       " 11,\n",
       " 1083,\n",
       " 11,\n",
       " 10339,\n",
       " 429,\n",
       " 807,\n",
       " 646,\n",
       " 387,\n",
       " 1483,\n",
       " 369,\n",
       " 5257,\n",
       " 9079,\n",
       " 13,\n",
       " 97593,\n",
       " 11,\n",
       " 429,\n",
       " 1265,\n",
       " 3421,\n",
       " 432,\n",
       " 624,\n",
       " 151668,\n",
       " 271,\n",
       " 32,\n",
       " 3460,\n",
       " 4128,\n",
       " 1614,\n",
       " 320,\n",
       " 4086,\n",
       " 44,\n",
       " 8,\n",
       " 374,\n",
       " 264,\n",
       " 943,\n",
       " 315,\n",
       " 20443,\n",
       " 11229,\n",
       " 1849,\n",
       " 6188,\n",
       " 311,\n",
       " 3535,\n",
       " 323,\n",
       " 6923,\n",
       " 3738,\n",
       " 12681,\n",
       " 1467,\n",
       " 13,\n",
       " 4220,\n",
       " 4119,\n",
       " 525,\n",
       " 16176,\n",
       " 389,\n",
       " 10951,\n",
       " 29425,\n",
       " 311,\n",
       " 57289,\n",
       " 6351,\n",
       " 4128,\n",
       " 11,\n",
       " 3535,\n",
       " 2266,\n",
       " 11,\n",
       " 323,\n",
       " 2736,\n",
       " 9079,\n",
       " 1741,\n",
       " 438,\n",
       " 4378,\n",
       " 11,\n",
       " 3412,\n",
       " 11,\n",
       " 323,\n",
       " 11521,\n",
       " 7274,\n",
       " 13,\n",
       " 2379,\n",
       " 7283,\n",
       " 3847,\n",
       " 311,\n",
       " 16282,\n",
       " 448,\n",
       " 4128,\n",
       " 304,\n",
       " 264,\n",
       " 5810,\n",
       " 323,\n",
       " 11050,\n",
       " 1616,\n",
       " 11,\n",
       " 3259,\n",
       " 1105,\n",
       " 55093,\n",
       " 369,\n",
       " 9079,\n",
       " 23994,\n",
       " 504,\n",
       " 2213,\n",
       " 9688,\n",
       " 311,\n",
       " 3491,\n",
       " 98146,\n",
       " 13,\n",
       " 151645]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_generated_ids = generated_ids[0].tolist()\n",
    "list_generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965977d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151667,\n",
       " 198,\n",
       " 32313,\n",
       " 11,\n",
       " 279,\n",
       " 1196,\n",
       " 6801,\n",
       " 264,\n",
       " 2805,\n",
       " 16800,\n",
       " 311,\n",
       " 264,\n",
       " 3460,\n",
       " 4128,\n",
       " 1614,\n",
       " 13,\n",
       " 6771,\n",
       " 752,\n",
       " 1191,\n",
       " 553,\n",
       " 88646,\n",
       " 1128,\n",
       " 358,\n",
       " 1414,\n",
       " 911,\n",
       " 444,\n",
       " 10994,\n",
       " 82,\n",
       " 13,\n",
       " 2379,\n",
       " 2299,\n",
       " 2409,\n",
       " 4128,\n",
       " 4119,\n",
       " 11,\n",
       " 1290,\n",
       " 30,\n",
       " 2055,\n",
       " 358,\n",
       " 1265,\n",
       " 6286,\n",
       " 862,\n",
       " 6200,\n",
       " 4419,\n",
       " 1156,\n",
       " 13,\n",
       " 10696,\n",
       " 1191,\n",
       " 448,\n",
       " 330,\n",
       " 34253,\n",
       " 4128,\n",
       " 4119,\n",
       " 320,\n",
       " 4086,\n",
       " 21634,\n",
       " 9940,\n",
       " 323,\n",
       " 10339,\n",
       " 862,\n",
       " 16928,\n",
       " 382,\n",
       " 40,\n",
       " 1184,\n",
       " 311,\n",
       " 11167,\n",
       " 1376,\n",
       " 13566,\n",
       " 1075,\n",
       " 23163,\n",
       " 1467,\n",
       " 11,\n",
       " 8660,\n",
       " 2266,\n",
       " 11,\n",
       " 323,\n",
       " 11589,\n",
       " 6351,\n",
       " 9079,\n",
       " 13,\n",
       " 7281,\n",
       " 11,\n",
       " 6286,\n",
       " 429,\n",
       " 807,\n",
       " 2299,\n",
       " 16176,\n",
       " 389,\n",
       " 12767,\n",
       " 29425,\n",
       " 13,\n",
       " 12260,\n",
       " 358,\n",
       " 2924,\n",
       " 2494,\n",
       " 911,\n",
       " 862,\n",
       " 8357,\n",
       " 30,\n",
       " 8909,\n",
       " 4378,\n",
       " 11,\n",
       " 3412,\n",
       " 11,\n",
       " 476,\n",
       " 11521,\n",
       " 9079,\n",
       " 30,\n",
       " 2938,\n",
       " 3643,\n",
       " 279,\n",
       " 16800,\n",
       " 803,\n",
       " 15817,\n",
       " 382,\n",
       " 14190,\n",
       " 11,\n",
       " 279,\n",
       " 1196,\n",
       " 3207,\n",
       " 944,\n",
       " 13837,\n",
       " 421,\n",
       " 807,\n",
       " 1184,\n",
       " 10916,\n",
       " 3793,\n",
       " 476,\n",
       " 264,\n",
       " 803,\n",
       " 4586,\n",
       " 16148,\n",
       " 13,\n",
       " 8704,\n",
       " 432,\n",
       " 594,\n",
       " 264,\n",
       " 2805,\n",
       " 19706,\n",
       " 11,\n",
       " 10282,\n",
       " 432,\n",
       " 63594,\n",
       " 374,\n",
       " 2989,\n",
       " 13,\n",
       " 34006,\n",
       " 502,\n",
       " 70821,\n",
       " 421,\n",
       " 3204,\n",
       " 13,\n",
       " 10696,\n",
       " 912,\n",
       " 264,\n",
       " 11652,\n",
       " 911,\n",
       " 862,\n",
       " 10515,\n",
       " 2897,\n",
       " 323,\n",
       " 59012,\n",
       " 13,\n",
       " 6771,\n",
       " 752,\n",
       " 1779,\n",
       " 421,\n",
       " 358,\n",
       " 2776,\n",
       " 7402,\n",
       " 894,\n",
       " 2989,\n",
       " 3501,\n",
       " 13,\n",
       " 8670,\n",
       " 11,\n",
       " 1083,\n",
       " 11,\n",
       " 10339,\n",
       " 429,\n",
       " 807,\n",
       " 646,\n",
       " 387,\n",
       " 1483,\n",
       " 369,\n",
       " 5257,\n",
       " 9079,\n",
       " 13,\n",
       " 97593,\n",
       " 11,\n",
       " 429,\n",
       " 1265,\n",
       " 3421,\n",
       " 432,\n",
       " 624,\n",
       " 151668,\n",
       " 271,\n",
       " 32,\n",
       " 3460,\n",
       " 4128,\n",
       " 1614,\n",
       " 320,\n",
       " 4086,\n",
       " 44,\n",
       " 8,\n",
       " 374,\n",
       " 264,\n",
       " 943,\n",
       " 315,\n",
       " 20443,\n",
       " 11229,\n",
       " 1849,\n",
       " 6188,\n",
       " 311,\n",
       " 3535,\n",
       " 323,\n",
       " 6923,\n",
       " 3738,\n",
       " 12681,\n",
       " 1467,\n",
       " 13,\n",
       " 4220,\n",
       " 4119,\n",
       " 525,\n",
       " 16176,\n",
       " 389,\n",
       " 10951,\n",
       " 29425,\n",
       " 311,\n",
       " 57289,\n",
       " 6351,\n",
       " 4128,\n",
       " 11,\n",
       " 3535,\n",
       " 2266,\n",
       " 11,\n",
       " 323,\n",
       " 2736,\n",
       " 9079,\n",
       " 1741,\n",
       " 438,\n",
       " 4378,\n",
       " 11,\n",
       " 3412,\n",
       " 11,\n",
       " 323,\n",
       " 11521,\n",
       " 7274,\n",
       " 13,\n",
       " 2379,\n",
       " 7283,\n",
       " 3847,\n",
       " 311,\n",
       " 16282,\n",
       " 448,\n",
       " 4128,\n",
       " 304,\n",
       " 264,\n",
       " 5810,\n",
       " 323,\n",
       " 11050,\n",
       " 1616,\n",
       " 11,\n",
       " 3259,\n",
       " 1105,\n",
       " 55093,\n",
       " 369,\n",
       " 9079,\n",
       " 23994,\n",
       " 504,\n",
       " 2213,\n",
       " 9688,\n",
       " 311,\n",
       " 3491,\n",
       " 98146,\n",
       " 13,\n",
       " 151645]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() # 去掉prompt部分, 并变成list\n",
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6311f777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, the user wants a short introduction to a large language model. Let me start by recalling what I know about LLMs. They're big language models, right? So I should mention their core features first. Maybe start with \"Large language models (LLMs)\" and explain their capabilities.\n",
      "\n",
      "I need to highlight key aspects like generating text, understanding context, and handling complex tasks. Also, mention that they're trained on vast datasets. Should I include something about their applications? Like writing, research, or creative tasks? That makes the introduction more comprehensive.\n",
      "\n",
      "Wait, the user didn't specify if they need technical terms or a more general explanation. Since it's a short intro, keeping it concise is important. Avoid jargon if possible. Maybe add a sentence about their adaptability and versatility. Let me check if I'm missing any important points. Oh, also, explain that they can be used for various tasks. Alright, that should cover it.\n",
      "</think>\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "content: A large language model (LLM) is a type of artificial intelligence system designed to understand and generate human-like text. These models are trained on massive datasets to comprehend complex language, understand context, and perform tasks such as writing, research, and creative thinking. They enable users to interact with language in a natural and efficient way, making them invaluable for tasks ranging from content creation to problem-solving.\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "full content: user\n",
      "Give me a short introduction to large language model.\n",
      "assistant\n",
      "<think>\n",
      "Okay, the user wants a short introduction to a large language model. Let me start by recalling what I know about LLMs. They're big language models, right? So I should mention their core features first. Maybe start with \"Large language models (LLMs)\" and explain their capabilities.\n",
      "\n",
      "I need to highlight key aspects like generating text, understanding context, and handling complex tasks. Also, mention that they're trained on vast datasets. Should I include something about their applications? Like writing, research, or creative tasks? That makes the introduction more comprehensive.\n",
      "\n",
      "Wait, the user didn't specify if they need technical terms or a more general explanation. Since it's a short intro, keeping it concise is important. Avoid jargon if possible. Maybe add a sentence about their adaptability and versatility. Let me check if I'm missing any important points. Oh, also, explain that they can be used for various tasks. Alright, that should cover it.\n",
      "</think>\n",
      "\n",
      "A large language model (LLM) is a type of artificial intelligence system designed to understand and generate human-like text. These models are trained on massive datasets to comprehend complex language, understand context, and perform tasks such as writing, research, and creative thinking. They enable users to interact with language in a natural and efficient way, making them invaluable for tasks ranging from content creation to problem-solving.\n"
     ]
    }
   ],
   "source": [
    "# 解析thinking的部分（如果开启thinking模式）\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "full_content = tokenizer.decode(list_generated_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"-----\"*40)\n",
    "print(\"content:\", content)\n",
    "print(\"-----\"*40)\n",
    "print(\"full content:\", full_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e263870e",
   "metadata": {},
   "source": [
    "# Back\n",
    "\n",
    "pip install uvicorn\n",
    "\n",
    "pip install fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d420eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [19452]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:5000 (Press CTRL+C to quit)\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [19452]\n"
     ]
    }
   ],
   "source": [
    "import uvicorn\n",
    "from fastapi import FastAPI,Body\n",
    "from fastapi.responses import JSONResponse\n",
    "from typing import Dict\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "def chat(prompt: str|int):\n",
    "    messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=True # 切换是否为思考模式\n",
    "        )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device) \n",
    "    generated = model.generate(\n",
    "        **model_inputs,\n",
    "        max_length=32768,\n",
    "        num_return_sequences=1\n",
    "        )\n",
    "    output = generated[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output) - output[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "\n",
    "    thinking_content = tokenizer.decode(output[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    full_content = tokenizer.decode(output, skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "    # print(\"thinking content:\", thinking_content)\n",
    "    # print(\"-----\"*40)\n",
    "    # print(\"content:\", content)\n",
    "    # print(\"-----\"*40)\n",
    "    # print(\"full content:\", full_content)\n",
    "\n",
    "    response = {'thinking content':thinking_content,'content':content,'full content':full_content}\n",
    "    return JSONResponse(content=response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
    "    # 下面这是针对在jupyter notebook里启动uvicorn的\n",
    "    # 否则会报错：RuntimeError: asyncio.run() cannot be called from a running event loop\n",
    "    config = uvicorn.Config(app, port=5000, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    await server.serve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8371ad",
   "metadata": {},
   "source": [
    "输入 ： Give me a short introduction to large language model.\n",
    "\n",
    "response里的curl：\n",
    "\n",
    "curl -X 'POST' \\\n",
    "  'http://127.0.0.1:5000/chat?prompt=Give%20me%20a%20short%20introduction%20to%20large%20language%20model.' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -d ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a431e",
   "metadata": {},
   "source": [
    "# Front\n",
    "\n",
    "用到 langchain\n",
    "\n",
    "pip install langchain\n",
    "pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78cc975e",
   "metadata": {},
   "outputs": [
    {
     "ename": "PydanticUserError",
     "evalue": "A non-annotated attribute was detected: `url = 'http://127.0.0.1:5000/chat'`. All model fields require a type annotation; if `url` is not meant to be a field, you may be able to resolve this error by annotating it as a `ClassVar` or updating `model_config['ignored_types']`.\n\nFor further information visit https://errors.pydantic.dev/2.11/u/model-field-missing-annotation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPydanticUserError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[0;32m      9\u001b[0m langchain\u001b[38;5;241m.\u001b[39mllm_cache \u001b[38;5;241m=\u001b[39m InMemoryCache()\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mChatLLM\u001b[39;00m(LLM):\n\u001b[0;32m     13\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:5000/chat\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 换成上一栏输出的uvicorn地址\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\clip\\lib\\site-packages\\pydantic\\_internal\\_model_construction.py:112\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[1;34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m config_wrapper \u001b[38;5;241m=\u001b[39m ConfigWrapper\u001b[38;5;241m.\u001b[39mfor_model(bases, namespace, kwargs)\n\u001b[0;32m    111\u001b[0m namespace[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_config\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m config_wrapper\u001b[38;5;241m.\u001b[39mconfig_dict\n\u001b[1;32m--> 112\u001b[0m private_attributes \u001b[38;5;241m=\u001b[39m \u001b[43minspect_namespace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignored_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_field_names\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m private_attributes \u001b[38;5;129;01mor\u001b[39;00m base_private_attributes:\n\u001b[0;32m    116\u001b[0m     original_model_post_init \u001b[38;5;241m=\u001b[39m get_model_post_init(namespace, bases)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\clip\\lib\\site-packages\\pydantic\\_internal\\_model_construction.py:459\u001b[0m, in \u001b[0;36minspect_namespace\u001b[1;34m(namespace, ignored_types, base_class_vars, base_class_fields)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[0;32m    456\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mField \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m requires a type annotation\u001b[39m\u001b[38;5;124m'\u001b[39m, code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel-field-missing-annotation\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    457\u001b[0m             )\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[0;32m    460\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA non-annotated attribute was detected: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m`. All model fields require a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    461\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype annotation; if `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is not meant to be a field, you may be able to resolve this \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    462\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror by annotating it as a `ClassVar` or updating `model_config[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignored_types\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]`.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    463\u001b[0m                 code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel-field-missing-annotation\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    464\u001b[0m             )\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ann_name, ann_type \u001b[38;5;129;01min\u001b[39;00m raw_annotations\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    468\u001b[0m         is_valid_privateattr_name(ann_name)\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m ann_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m private_attributes\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ann_type, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__module__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunctools\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    476\u001b[0m     ):\n",
      "\u001b[1;31mPydanticUserError\u001b[0m: A non-annotated attribute was detected: `url = 'http://127.0.0.1:5000/chat'`. All model fields require a type annotation; if `url` is not meant to be a field, you may be able to resolve this error by annotating it as a `ClassVar` or updating `model_config['ignored_types']`.\n\nFor further information visit https://errors.pydantic.dev/2.11/u/model-field-missing-annotation"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import logging\n",
    "from typing import Optional, List, Dict, Mapping, Any\n",
    "import langchain\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "\n",
    "class ChatLLM(LLM):\n",
    "    url = \"http://127.0.0.1:5000/chat\"  # 换成上一栏输出的uvicorn地址\n",
    "    history = []\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"qwen-3.0\"  # 修改为对应模型名称\n",
    "\n",
    "    def _construct_query(self, prompt: str) -> Dict:\n",
    "        query = {\n",
    "            \"history\": self.history,\n",
    "            \"query\": prompt\n",
    "        }\n",
    "        import json\n",
    "        query = json.dumps(query)\n",
    "        return query\n",
    "\n",
    "    @classmethod\n",
    "    def _post(self, url: str, query: Dict) -> Any:\n",
    "        response = requests.post(url, data=query).json()\n",
    "        return response\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        query = self._construct_query(prompt=prompt)\n",
    "        response = self._post(url=self.url, query=query)\n",
    "        response_chat = response['response']\n",
    "        self.history = response['history']\n",
    "        return response_chat\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        _param_dict = {\n",
    "            \"url\": self.url\n",
    "        }\n",
    "        return _param_dict\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    llm = ChatLLM()\n",
    "    while True:\n",
    "        user_input   = input(\"我: \")\n",
    "        response = llm(user_input)\n",
    "        print(f\"ChatGLM: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c8422",
   "metadata": {},
   "source": [
    "# Web / APP 框架\n",
    "\n",
    "pip install gradio\n",
    "\n",
    "#如果安装gradio后ImportError: DLL load failed while importing _multiarray_umath: 找不到指定的模块。执行下面：\n",
    " pip install numpy==1.25.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from client import ChatLLM #引用client.py里面我们定义的ChatLLM\n",
    "\n",
    "llm = ChatLLM()\n",
    "# 流式处理\n",
    "def stream_translate(text):\n",
    "    response = llm(text)\n",
    "    for chunk in response.split():\n",
    "        yield chunk + \" \"\n",
    "\n",
    "\n",
    "demo = gr.Interface(fn=stream_translate, inputs=\"text\", outputs=\"text\", title=\"ChatGLM\",\n",
    "                    description=\"A chatbot powered by ChatGLM.\")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
